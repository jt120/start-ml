{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open('d:/tmp/movie_data.csv.gz') as f_in, open('movie_data.csv', 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:05:38\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Counting words occurences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 1974 ,  the teenager martha moxley  ( maggie grace )  moves to the high - class area of belle haven ,  greenwich ,  connecticut .  on the mischief night ,  eve of halloween ,  she was murdered in the backyard of her house and her murder remained unsolved .  twenty - two years later ,  the writer mark fuhrman  ( christopher meloni )  ,  who is a former la detective that has fallen in disgrace for perjury in o . j .  simpson trial and moved to idaho ,  decides to investigate the case with his partner stephen weeks  ( andrew mitchell )  with the purpose of writing a book .  the locals squirm and do not welcome them ,  but with the support of the retired detective steve carroll  ( robert forster )  that was in charge of the investigation in the 70 \\' s ,  they discover the criminal and a net of power and money to cover the murder .  < br  /  >  < br  /  >  \" murder in greenwich \"  is a good tv movie ,  with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy .  the powerful and rich family used their influence to cover the murder for more than twenty years .  however ,  a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed .  the screenplay shows the investigation of mark and the last days of martha in parallel ,  but there is a lack of the emotion in the dramatization .  my vote is seven .  < br  /  >  < br  /  > title  ( brazil )  :  not available'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    }
   ],
   "source": [
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penalised', 'adelade', 'bunki', 'explitive', 'damnedest', 'kitchenette', 'mechagodzilla', 'gamekeeper', 'synapsis', 'earlies', 'sagacity', 'himnan', 'shultz', 'tinian', 'movie\\x97my']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102952"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_counts[-15:])\n",
    "word_to_int['penalised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:10\n"
     ]
    }
   ],
   "source": [
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']),\n",
    "                       title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapped_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length = 200  ## sequence length (or T in our formulas)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
    "\n",
    "X_train = sequences[:25000, :]\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = sequences[25000:, :]\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "\n",
    "## Function to generate minibatches:\n",
    "def create_batch_generator(x, y=None, batch_size=64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x= x[:n_batches*batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "        else:\n",
    "            yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class SentimentRNN(object):\n",
    "    def __init__(self, n_words, seq_len=200,\n",
    "                 lstm_size=256, num_layers=1, batch_size=64,\n",
    "                 learning_rate=0.0001, embed_size=200):\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size   ## number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "\n",
    "    def build(self):\n",
    "        ## Define the placeholders\n",
    "        tf_x = tf.placeholder(tf.int32,\n",
    "                    shape=(self.batch_size, self.seq_len),\n",
    "                    name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32,\n",
    "                    shape=(self.batch_size),\n",
    "                    name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32,\n",
    "                    name='tf_keepprob')\n",
    "        ## Create the embedding layer\n",
    "        embedding = tf.Variable(\n",
    "                    tf.random_uniform(\n",
    "                        (self.n_words, self.embed_size),\n",
    "                        minval=-1, maxval=1),\n",
    "                    name='embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(\n",
    "                    embedding, tf_x, \n",
    "                    name='embeded_x')\n",
    "\n",
    "        ## Define LSTM cell and stack them together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "                [tf.contrib.rnn.DropoutWrapper(\n",
    "                   tf.contrib.rnn.BasicLSTMCell(self.lstm_size),\n",
    "                   output_keep_prob=tf_keepprob)\n",
    "                 for i in range(self.num_layers)])\n",
    "\n",
    "        ## Define the initial state:\n",
    "        self.initial_state = cells.zero_state(\n",
    "                 self.batch_size, tf.float32)\n",
    "        print('  << initial state >> ', self.initial_state)\n",
    "\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                 cells, embed_x,\n",
    "                 initial_state=self.initial_state)\n",
    "        ## Note: lstm_outputs shape: \n",
    "        ##  [batch_size, max_time, cells.output_size]\n",
    "        print('\\n  << lstm_output   >> ', lstm_outputs)\n",
    "        print('\\n  << final state   >> ', self.final_state)\n",
    "\n",
    "        ## Apply a FC layer after on top of RNN output:\n",
    "        logits = tf.layers.dense(\n",
    "                 inputs=lstm_outputs[:, -1],\n",
    "                 units=1, activation=None,\n",
    "                 name='logits')\n",
    "        \n",
    "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
    "        print ('\\n  << logits        >> ', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
    "        predictions = {\n",
    "            'probabilities': y_proba,\n",
    "            'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
    "                 name='labels')\n",
    "        }\n",
    "        print('\\n  << predictions   >> ', predictions)\n",
    "\n",
    "        ## Define the cost function\n",
    "        cost = tf.reduce_mean(\n",
    "                 tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                 labels=tf_y, logits=logits),\n",
    "                 name='cost')\n",
    "        \n",
    "        ## Define the optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name='train_op')\n",
    "\n",
    "    def train(self, X_train, y_train, num_epochs):\n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "            iteration = 1\n",
    "            for epoch in range(num_epochs):\n",
    "                state = sess.run(self.initial_state)\n",
    "                \n",
    "                for batch_x, batch_y in create_batch_generator(\n",
    "                            X_train, y_train, self.batch_size):\n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': 0.5,\n",
    "                            self.initial_state : state}\n",
    "                    loss, _, state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                             self.final_state],\n",
    "                            feed_dict=feed)\n",
    "\n",
    "                    if iteration % 20 == 0:\n",
    "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
    "                              \"| Train loss: %.5f\" % (\n",
    "                               epoch + 1, num_epochs,\n",
    "                               iteration, loss))\n",
    "\n",
    "                    iteration +=1\n",
    "                if (epoch+1)%10 == 0:\n",
    "                    self.saver.save(sess,\n",
    "                        \"model/sentiment-%d.ckpt\" % epoch)\n",
    "\n",
    "    def predict(self, X_data, return_proba=False):\n",
    "        preds = []\n",
    "        with tf.Session(graph = self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, tf.train.latest_checkpoint('model/'))\n",
    "            test_state = sess.run(self.initial_state)\n",
    "            for ii, batch_x in enumerate(\n",
    "                create_batch_generator(\n",
    "                    X_data, None, batch_size=self.batch_size), 1):\n",
    "                feed = {'tf_x:0' : batch_x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state : test_state}\n",
    "                if return_proba:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                else:\n",
    "                    pred, test_state = sess.run(\n",
    "                        ['labels:0', self.final_state],\n",
    "                        feed_dict=feed)\n",
    "                    \n",
    "                preds.append(pred)\n",
    "                \n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << lstm_output   >>  Tensor(\"rnn/transpose:0\", shape=(100, 200, 128), dtype=float32)\n",
      "\n",
      "  << final state   >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_2:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>),)\n",
      "\n",
      "  << logits        >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
      "\n",
      "  << predictions   >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
     ]
    }
   ],
   "source": [
    "n_words = max(list(word_to_int.values())) + 1\n",
    "\n",
    "rnn = SentimentRNN(n_words=n_words, \n",
    "                   seq_len=sequence_length,\n",
    "                   embed_size=256, \n",
    "                   lstm_size=128, \n",
    "                   num_layers=1, \n",
    "                   batch_size=100, \n",
    "                   learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 Iteration: 20 | Train loss: 0.70424\n",
      "Epoch: 1/40 Iteration: 40 | Train loss: 0.61183\n",
      "Epoch: 1/40 Iteration: 60 | Train loss: 0.61106\n",
      "Epoch: 1/40 Iteration: 80 | Train loss: 0.53204\n",
      "Epoch: 1/40 Iteration: 100 | Train loss: 0.59602\n",
      "Epoch: 1/40 Iteration: 120 | Train loss: 0.60881\n",
      "Epoch: 1/40 Iteration: 140 | Train loss: 0.60886\n",
      "Epoch: 1/40 Iteration: 160 | Train loss: 0.49927\n",
      "Epoch: 1/40 Iteration: 180 | Train loss: 0.45666\n",
      "Epoch: 1/40 Iteration: 200 | Train loss: 0.47791\n",
      "Epoch: 1/40 Iteration: 220 | Train loss: 0.34807\n",
      "Epoch: 1/40 Iteration: 240 | Train loss: 0.47828\n",
      "Epoch: 2/40 Iteration: 260 | Train loss: 0.45152\n",
      "Epoch: 2/40 Iteration: 280 | Train loss: 0.34978\n",
      "Epoch: 2/40 Iteration: 300 | Train loss: 0.35281\n",
      "Epoch: 2/40 Iteration: 320 | Train loss: 0.42379\n",
      "Epoch: 2/40 Iteration: 340 | Train loss: 0.37396\n",
      "Epoch: 2/40 Iteration: 360 | Train loss: 0.23252\n",
      "Epoch: 2/40 Iteration: 380 | Train loss: 0.34016\n",
      "Epoch: 2/40 Iteration: 400 | Train loss: 0.31848\n",
      "Epoch: 2/40 Iteration: 420 | Train loss: 0.32594\n",
      "Epoch: 2/40 Iteration: 440 | Train loss: 0.29692\n",
      "Epoch: 2/40 Iteration: 460 | Train loss: 0.40545\n",
      "Epoch: 2/40 Iteration: 480 | Train loss: 0.30537\n",
      "Epoch: 2/40 Iteration: 500 | Train loss: 0.21484\n",
      "Epoch: 3/40 Iteration: 520 | Train loss: 0.30231\n",
      "Epoch: 3/40 Iteration: 540 | Train loss: 0.22195\n",
      "Epoch: 3/40 Iteration: 560 | Train loss: 0.33078\n",
      "Epoch: 3/40 Iteration: 580 | Train loss: 0.20907\n",
      "Epoch: 3/40 Iteration: 600 | Train loss: 0.20347\n",
      "Epoch: 3/40 Iteration: 620 | Train loss: 0.21980\n",
      "Epoch: 3/40 Iteration: 640 | Train loss: 0.21325\n",
      "Epoch: 3/40 Iteration: 660 | Train loss: 0.19944\n",
      "Epoch: 3/40 Iteration: 680 | Train loss: 0.27544\n",
      "Epoch: 3/40 Iteration: 700 | Train loss: 0.17961\n",
      "Epoch: 3/40 Iteration: 720 | Train loss: 0.30727\n",
      "Epoch: 3/40 Iteration: 740 | Train loss: 0.29388\n",
      "Epoch: 4/40 Iteration: 760 | Train loss: 0.32948\n",
      "Epoch: 4/40 Iteration: 780 | Train loss: 0.14631\n",
      "Epoch: 4/40 Iteration: 800 | Train loss: 0.24968\n",
      "Epoch: 4/40 Iteration: 820 | Train loss: 0.29956\n",
      "Epoch: 4/40 Iteration: 840 | Train loss: 0.13340\n",
      "Epoch: 4/40 Iteration: 860 | Train loss: 0.12557\n",
      "Epoch: 4/40 Iteration: 880 | Train loss: 0.21185\n",
      "Epoch: 4/40 Iteration: 900 | Train loss: 0.22761\n",
      "Epoch: 4/40 Iteration: 920 | Train loss: 0.20540\n",
      "Epoch: 4/40 Iteration: 940 | Train loss: 0.21112\n",
      "Epoch: 4/40 Iteration: 960 | Train loss: 0.10475\n",
      "Epoch: 4/40 Iteration: 980 | Train loss: 0.19961\n",
      "Epoch: 4/40 Iteration: 1000 | Train loss: 0.17185\n",
      "Epoch: 5/40 Iteration: 1020 | Train loss: 0.15091\n",
      "Epoch: 5/40 Iteration: 1040 | Train loss: 0.11403\n",
      "Epoch: 5/40 Iteration: 1060 | Train loss: 0.11919\n",
      "Epoch: 5/40 Iteration: 1080 | Train loss: 0.06159\n",
      "Epoch: 5/40 Iteration: 1100 | Train loss: 0.07042\n",
      "Epoch: 5/40 Iteration: 1120 | Train loss: 0.11693\n",
      "Epoch: 5/40 Iteration: 1140 | Train loss: 0.04947\n",
      "Epoch: 5/40 Iteration: 1160 | Train loss: 0.08931\n",
      "Epoch: 5/40 Iteration: 1180 | Train loss: 0.15010\n",
      "Epoch: 5/40 Iteration: 1200 | Train loss: 0.10446\n",
      "Epoch: 5/40 Iteration: 1220 | Train loss: 0.10561\n",
      "Epoch: 5/40 Iteration: 1240 | Train loss: 0.13826\n",
      "Epoch: 6/40 Iteration: 1260 | Train loss: 0.12773\n",
      "Epoch: 6/40 Iteration: 1280 | Train loss: 0.04891\n",
      "Epoch: 6/40 Iteration: 1300 | Train loss: 0.06573\n",
      "Epoch: 6/40 Iteration: 1320 | Train loss: 0.12707\n",
      "Epoch: 6/40 Iteration: 1340 | Train loss: 0.03880\n",
      "Epoch: 6/40 Iteration: 1360 | Train loss: 0.01462\n",
      "Epoch: 6/40 Iteration: 1380 | Train loss: 0.14458\n",
      "Epoch: 6/40 Iteration: 1400 | Train loss: 0.19603\n",
      "Epoch: 6/40 Iteration: 1420 | Train loss: 0.13234\n",
      "Epoch: 6/40 Iteration: 1440 | Train loss: 0.12080\n",
      "Epoch: 6/40 Iteration: 1460 | Train loss: 0.09918\n",
      "Epoch: 6/40 Iteration: 1480 | Train loss: 0.32643\n",
      "Epoch: 6/40 Iteration: 1500 | Train loss: 0.18917\n",
      "Epoch: 7/40 Iteration: 1520 | Train loss: 0.11358\n",
      "Epoch: 7/40 Iteration: 1540 | Train loss: 0.04186\n",
      "Epoch: 7/40 Iteration: 1560 | Train loss: 0.15149\n",
      "Epoch: 7/40 Iteration: 1580 | Train loss: 0.08349\n",
      "Epoch: 7/40 Iteration: 1600 | Train loss: 0.07196\n",
      "Epoch: 7/40 Iteration: 1620 | Train loss: 0.06855\n",
      "Epoch: 7/40 Iteration: 1640 | Train loss: 0.03850\n",
      "Epoch: 7/40 Iteration: 1660 | Train loss: 0.02402\n",
      "Epoch: 7/40 Iteration: 1680 | Train loss: 0.07134\n",
      "Epoch: 7/40 Iteration: 1700 | Train loss: 0.02482\n",
      "Epoch: 7/40 Iteration: 1720 | Train loss: 0.06013\n",
      "Epoch: 7/40 Iteration: 1740 | Train loss: 0.12926\n",
      "Epoch: 8/40 Iteration: 1760 | Train loss: 0.18766\n",
      "Epoch: 8/40 Iteration: 1780 | Train loss: 0.02377\n",
      "Epoch: 8/40 Iteration: 1800 | Train loss: 0.06649\n",
      "Epoch: 8/40 Iteration: 1820 | Train loss: 0.07428\n",
      "Epoch: 8/40 Iteration: 1840 | Train loss: 0.02136\n",
      "Epoch: 8/40 Iteration: 1860 | Train loss: 0.03109\n",
      "Epoch: 8/40 Iteration: 1880 | Train loss: 0.06121\n",
      "Epoch: 8/40 Iteration: 1900 | Train loss: 0.05645\n",
      "Epoch: 8/40 Iteration: 1920 | Train loss: 0.06811\n",
      "Epoch: 8/40 Iteration: 1940 | Train loss: 0.11794\n",
      "Epoch: 8/40 Iteration: 1960 | Train loss: 0.04167\n",
      "Epoch: 8/40 Iteration: 1980 | Train loss: 0.09831\n",
      "Epoch: 8/40 Iteration: 2000 | Train loss: 0.12413\n",
      "Epoch: 9/40 Iteration: 2020 | Train loss: 0.10441\n",
      "Epoch: 9/40 Iteration: 2040 | Train loss: 0.04189\n",
      "Epoch: 9/40 Iteration: 2060 | Train loss: 0.03810\n",
      "Epoch: 9/40 Iteration: 2080 | Train loss: 0.06977\n",
      "Epoch: 9/40 Iteration: 2100 | Train loss: 0.04174\n",
      "Epoch: 9/40 Iteration: 2120 | Train loss: 0.05089\n",
      "Epoch: 9/40 Iteration: 2140 | Train loss: 0.10769\n",
      "Epoch: 9/40 Iteration: 2160 | Train loss: 0.01194\n",
      "Epoch: 9/40 Iteration: 2180 | Train loss: 0.07675\n",
      "Epoch: 9/40 Iteration: 2200 | Train loss: 0.00945\n",
      "Epoch: 9/40 Iteration: 2220 | Train loss: 0.03848\n",
      "Epoch: 9/40 Iteration: 2240 | Train loss: 0.04055\n",
      "Epoch: 10/40 Iteration: 2260 | Train loss: 0.01193\n",
      "Epoch: 10/40 Iteration: 2280 | Train loss: 0.02496\n",
      "Epoch: 10/40 Iteration: 2300 | Train loss: 0.04737\n",
      "Epoch: 10/40 Iteration: 2320 | Train loss: 0.08794\n",
      "Epoch: 10/40 Iteration: 2340 | Train loss: 0.01397\n",
      "Epoch: 10/40 Iteration: 2360 | Train loss: 0.01799\n",
      "Epoch: 10/40 Iteration: 2380 | Train loss: 0.01694\n",
      "Epoch: 10/40 Iteration: 2400 | Train loss: 0.01841\n",
      "Epoch: 10/40 Iteration: 2420 | Train loss: 0.02500\n",
      "Epoch: 10/40 Iteration: 2440 | Train loss: 0.05876\n",
      "Epoch: 10/40 Iteration: 2460 | Train loss: 0.00889\n",
      "Epoch: 10/40 Iteration: 2480 | Train loss: 0.03910\n",
      "Epoch: 10/40 Iteration: 2500 | Train loss: 0.14413\n",
      "Epoch: 11/40 Iteration: 2520 | Train loss: 0.01286\n",
      "Epoch: 11/40 Iteration: 2540 | Train loss: 0.01698\n",
      "Epoch: 11/40 Iteration: 2560 | Train loss: 0.05633\n",
      "Epoch: 11/40 Iteration: 2580 | Train loss: 0.02630\n",
      "Epoch: 11/40 Iteration: 2600 | Train loss: 0.02911\n",
      "Epoch: 11/40 Iteration: 2620 | Train loss: 0.01950\n",
      "Epoch: 11/40 Iteration: 2640 | Train loss: 0.00536\n",
      "Epoch: 11/40 Iteration: 2660 | Train loss: 0.03230\n",
      "Epoch: 11/40 Iteration: 2680 | Train loss: 0.02617\n",
      "Epoch: 11/40 Iteration: 2700 | Train loss: 0.01302\n",
      "Epoch: 11/40 Iteration: 2720 | Train loss: 0.24379\n",
      "Epoch: 11/40 Iteration: 2740 | Train loss: 0.05889\n",
      "Epoch: 12/40 Iteration: 2760 | Train loss: 0.02628\n",
      "Epoch: 12/40 Iteration: 2780 | Train loss: 0.00201\n",
      "Epoch: 12/40 Iteration: 2800 | Train loss: 0.01361\n",
      "Epoch: 12/40 Iteration: 2820 | Train loss: 0.02728\n",
      "Epoch: 12/40 Iteration: 2840 | Train loss: 0.00635\n",
      "Epoch: 12/40 Iteration: 2860 | Train loss: 0.02932\n",
      "Epoch: 12/40 Iteration: 2880 | Train loss: 0.03731\n",
      "Epoch: 12/40 Iteration: 2900 | Train loss: 0.05221\n",
      "Epoch: 12/40 Iteration: 2920 | Train loss: 0.00635\n",
      "Epoch: 12/40 Iteration: 2940 | Train loss: 0.03244\n",
      "Epoch: 12/40 Iteration: 2960 | Train loss: 0.00258\n",
      "Epoch: 12/40 Iteration: 2980 | Train loss: 0.03248\n",
      "Epoch: 12/40 Iteration: 3000 | Train loss: 0.08441\n",
      "Epoch: 13/40 Iteration: 3020 | Train loss: 0.00965\n",
      "Epoch: 13/40 Iteration: 3040 | Train loss: 0.00250\n",
      "Epoch: 13/40 Iteration: 3060 | Train loss: 0.00427\n",
      "Epoch: 13/40 Iteration: 3080 | Train loss: 0.01306\n",
      "Epoch: 13/40 Iteration: 3100 | Train loss: 0.02269\n",
      "Epoch: 13/40 Iteration: 3120 | Train loss: 0.02755\n",
      "Epoch: 13/40 Iteration: 3140 | Train loss: 0.00597\n",
      "Epoch: 13/40 Iteration: 3160 | Train loss: 0.03460\n",
      "Epoch: 13/40 Iteration: 3180 | Train loss: 0.04567\n",
      "Epoch: 13/40 Iteration: 3200 | Train loss: 0.03077\n",
      "Epoch: 13/40 Iteration: 3220 | Train loss: 0.00506\n",
      "Epoch: 13/40 Iteration: 3240 | Train loss: 0.01426\n",
      "Epoch: 14/40 Iteration: 3260 | Train loss: 0.02584\n",
      "Epoch: 14/40 Iteration: 3280 | Train loss: 0.00630\n",
      "Epoch: 14/40 Iteration: 3300 | Train loss: 0.00920\n",
      "Epoch: 14/40 Iteration: 3320 | Train loss: 0.00610\n",
      "Epoch: 14/40 Iteration: 3340 | Train loss: 0.00763\n",
      "Epoch: 14/40 Iteration: 3360 | Train loss: 0.00839\n",
      "Epoch: 14/40 Iteration: 3380 | Train loss: 0.02747\n",
      "Epoch: 14/40 Iteration: 3400 | Train loss: 0.01413\n",
      "Epoch: 14/40 Iteration: 3420 | Train loss: 0.01748\n",
      "Epoch: 14/40 Iteration: 3440 | Train loss: 0.00490\n",
      "Epoch: 14/40 Iteration: 3460 | Train loss: 0.01617\n",
      "Epoch: 14/40 Iteration: 3480 | Train loss: 0.00278\n",
      "Epoch: 14/40 Iteration: 3500 | Train loss: 0.12844\n",
      "Epoch: 15/40 Iteration: 3520 | Train loss: 0.00547\n",
      "Epoch: 15/40 Iteration: 3540 | Train loss: 0.00136\n",
      "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00189\n",
      "Epoch: 15/40 Iteration: 3580 | Train loss: 0.04200\n",
      "Epoch: 15/40 Iteration: 3600 | Train loss: 0.01011\n",
      "Epoch: 15/40 Iteration: 3620 | Train loss: 0.02527\n",
      "Epoch: 15/40 Iteration: 3640 | Train loss: 0.05749\n",
      "Epoch: 15/40 Iteration: 3660 | Train loss: 0.01403\n",
      "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00533\n",
      "Epoch: 15/40 Iteration: 3700 | Train loss: 0.01574\n",
      "Epoch: 15/40 Iteration: 3720 | Train loss: 0.00189\n",
      "Epoch: 15/40 Iteration: 3740 | Train loss: 0.00268\n",
      "Epoch: 16/40 Iteration: 3760 | Train loss: 0.00093\n",
      "Epoch: 16/40 Iteration: 3780 | Train loss: 0.00288\n",
      "Epoch: 16/40 Iteration: 3800 | Train loss: 0.01047\n",
      "Epoch: 16/40 Iteration: 3820 | Train loss: 0.00107\n",
      "Epoch: 16/40 Iteration: 3840 | Train loss: 0.04477\n",
      "Epoch: 16/40 Iteration: 3860 | Train loss: 0.00152\n",
      "Epoch: 16/40 Iteration: 3880 | Train loss: 0.03170\n",
      "Epoch: 16/40 Iteration: 3900 | Train loss: 0.00194\n",
      "Epoch: 16/40 Iteration: 3920 | Train loss: 0.00212\n",
      "Epoch: 16/40 Iteration: 3940 | Train loss: 0.00142\n",
      "Epoch: 16/40 Iteration: 3960 | Train loss: 0.00042\n",
      "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00134\n",
      "Epoch: 16/40 Iteration: 4000 | Train loss: 0.01259\n",
      "Epoch: 17/40 Iteration: 4020 | Train loss: 0.00089\n",
      "Epoch: 17/40 Iteration: 4040 | Train loss: 0.00109\n",
      "Epoch: 17/40 Iteration: 4060 | Train loss: 0.00090\n",
      "Epoch: 17/40 Iteration: 4080 | Train loss: 0.05377\n",
      "Epoch: 17/40 Iteration: 4100 | Train loss: 0.00169\n",
      "Epoch: 17/40 Iteration: 4120 | Train loss: 0.00118\n",
      "Epoch: 17/40 Iteration: 4140 | Train loss: 0.00066\n",
      "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00237\n",
      "Epoch: 17/40 Iteration: 4180 | Train loss: 0.00080\n",
      "Epoch: 17/40 Iteration: 4200 | Train loss: 0.00060\n",
      "Epoch: 17/40 Iteration: 4220 | Train loss: 0.00071\n",
      "Epoch: 17/40 Iteration: 4240 | Train loss: 0.00414\n",
      "Epoch: 18/40 Iteration: 4260 | Train loss: 0.00098\n",
      "Epoch: 18/40 Iteration: 4280 | Train loss: 0.00242\n",
      "Epoch: 18/40 Iteration: 4300 | Train loss: 0.00072\n",
      "Epoch: 18/40 Iteration: 4320 | Train loss: 0.00038\n",
      "Epoch: 18/40 Iteration: 4340 | Train loss: 0.01033\n",
      "Epoch: 18/40 Iteration: 4360 | Train loss: 0.00165\n",
      "Epoch: 18/40 Iteration: 4380 | Train loss: 0.00068\n",
      "Epoch: 18/40 Iteration: 4400 | Train loss: 0.00511\n",
      "Epoch: 18/40 Iteration: 4420 | Train loss: 0.00145\n",
      "Epoch: 18/40 Iteration: 4440 | Train loss: 0.00350\n",
      "Epoch: 18/40 Iteration: 4460 | Train loss: 0.00505\n",
      "Epoch: 18/40 Iteration: 4480 | Train loss: 0.00387\n",
      "Epoch: 18/40 Iteration: 4500 | Train loss: 0.00481\n",
      "Epoch: 19/40 Iteration: 4520 | Train loss: 0.00027\n",
      "Epoch: 19/40 Iteration: 4540 | Train loss: 0.00080\n",
      "Epoch: 19/40 Iteration: 4560 | Train loss: 0.00512\n",
      "Epoch: 19/40 Iteration: 4580 | Train loss: 0.00057\n",
      "Epoch: 19/40 Iteration: 4600 | Train loss: 0.00074\n",
      "Epoch: 19/40 Iteration: 4620 | Train loss: 0.00246\n",
      "Epoch: 19/40 Iteration: 4640 | Train loss: 0.00028\n",
      "Epoch: 19/40 Iteration: 4660 | Train loss: 0.00034\n",
      "Epoch: 19/40 Iteration: 4680 | Train loss: 0.00034\n",
      "Epoch: 19/40 Iteration: 4700 | Train loss: 0.00027\n",
      "Epoch: 19/40 Iteration: 4720 | Train loss: 0.00031\n",
      "Epoch: 19/40 Iteration: 4740 | Train loss: 0.00018\n",
      "Epoch: 20/40 Iteration: 4760 | Train loss: 0.00018\n",
      "Epoch: 20/40 Iteration: 4780 | Train loss: 0.00022\n",
      "Epoch: 20/40 Iteration: 4800 | Train loss: 0.00043\n",
      "Epoch: 20/40 Iteration: 4820 | Train loss: 0.00056\n",
      "Epoch: 20/40 Iteration: 4840 | Train loss: 0.00023\n",
      "Epoch: 20/40 Iteration: 4860 | Train loss: 0.00020\n",
      "Epoch: 20/40 Iteration: 4880 | Train loss: 0.00095\n",
      "Epoch: 20/40 Iteration: 4900 | Train loss: 0.00268\n",
      "Epoch: 20/40 Iteration: 4920 | Train loss: 0.00016\n",
      "Epoch: 20/40 Iteration: 4940 | Train loss: 0.00038\n",
      "Epoch: 20/40 Iteration: 4960 | Train loss: 0.00019\n",
      "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00027\n",
      "Epoch: 20/40 Iteration: 5000 | Train loss: 0.00027\n",
      "Epoch: 21/40 Iteration: 5020 | Train loss: 0.00014\n",
      "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00004\n",
      "Epoch: 21/40 Iteration: 5060 | Train loss: 0.00015\n",
      "Epoch: 21/40 Iteration: 5080 | Train loss: 0.00013\n",
      "Epoch: 21/40 Iteration: 5100 | Train loss: 0.00026\n",
      "Epoch: 21/40 Iteration: 5120 | Train loss: 0.00021\n",
      "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00018\n",
      "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00007\n",
      "Epoch: 21/40 Iteration: 5180 | Train loss: 0.00008\n",
      "Epoch: 21/40 Iteration: 5200 | Train loss: 0.00006\n",
      "Epoch: 21/40 Iteration: 5220 | Train loss: 0.00009\n",
      "Epoch: 21/40 Iteration: 5240 | Train loss: 0.00005\n",
      "Epoch: 22/40 Iteration: 5260 | Train loss: 0.00008\n",
      "Epoch: 22/40 Iteration: 5280 | Train loss: 0.00011\n",
      "Epoch: 22/40 Iteration: 5300 | Train loss: 0.00018\n",
      "Epoch: 22/40 Iteration: 5320 | Train loss: 0.00022\n",
      "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00033\n",
      "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00020\n",
      "Epoch: 22/40 Iteration: 5380 | Train loss: 0.00025\n",
      "Epoch: 22/40 Iteration: 5400 | Train loss: 0.00005\n",
      "Epoch: 22/40 Iteration: 5420 | Train loss: 0.00002\n",
      "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00017\n",
      "Epoch: 22/40 Iteration: 5460 | Train loss: 0.00004\n",
      "Epoch: 22/40 Iteration: 5480 | Train loss: 0.00025\n",
      "Epoch: 22/40 Iteration: 5500 | Train loss: 0.00026\n",
      "Epoch: 23/40 Iteration: 5520 | Train loss: 0.00002\n",
      "Epoch: 23/40 Iteration: 5540 | Train loss: 0.00002\n",
      "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00012\n",
      "Epoch: 23/40 Iteration: 5580 | Train loss: 0.00006\n",
      "Epoch: 23/40 Iteration: 5600 | Train loss: 0.00014\n",
      "Epoch: 23/40 Iteration: 5620 | Train loss: 0.00004\n",
      "Epoch: 23/40 Iteration: 5640 | Train loss: 0.00001\n",
      "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00009\n",
      "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00016\n",
      "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00002\n",
      "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00009\n",
      "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00005\n",
      "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00010\n",
      "Epoch: 24/40 Iteration: 5780 | Train loss: 0.00014\n",
      "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00010\n",
      "Epoch: 24/40 Iteration: 5820 | Train loss: 0.00014\n",
      "Epoch: 24/40 Iteration: 5840 | Train loss: 0.00009\n",
      "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00029\n",
      "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00006\n",
      "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00004\n",
      "Epoch: 24/40 Iteration: 5920 | Train loss: 0.00007\n",
      "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00025\n",
      "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00002\n",
      "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00008\n",
      "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00020\n",
      "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00003\n",
      "Epoch: 25/40 Iteration: 6040 | Train loss: 0.00001\n",
      "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00005\n",
      "Epoch: 25/40 Iteration: 6080 | Train loss: 0.00003\n",
      "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00007\n",
      "Epoch: 25/40 Iteration: 6120 | Train loss: 0.00006\n",
      "Epoch: 25/40 Iteration: 6140 | Train loss: 0.00002\n",
      "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00007\n",
      "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00012\n",
      "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00001\n",
      "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00002\n",
      "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00007\n",
      "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00003\n",
      "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00004\n",
      "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00002\n",
      "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00002\n",
      "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00005\n",
      "Epoch: 26/40 Iteration: 6360 | Train loss: 0.00002\n",
      "Epoch: 26/40 Iteration: 6380 | Train loss: 0.00006\n",
      "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00004\n",
      "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00002\n",
      "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00003\n",
      "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00001\n",
      "Epoch: 26/40 Iteration: 6480 | Train loss: 0.00003\n",
      "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00014\n",
      "Epoch: 27/40 Iteration: 6520 | Train loss: 0.00002\n",
      "Epoch: 27/40 Iteration: 6540 | Train loss: 0.00002\n",
      "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00002\n",
      "Epoch: 27/40 Iteration: 6580 | Train loss: 0.00001\n",
      "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00006\n",
      "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00002\n",
      "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00001\n",
      "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00004\n",
      "Epoch: 27/40 Iteration: 6680 | Train loss: 0.00002\n",
      "Epoch: 27/40 Iteration: 6700 | Train loss: 0.00001\n",
      "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00001\n",
      "Epoch: 27/40 Iteration: 6740 | Train loss: 0.00001\n",
      "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00000\n",
      "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00002\n",
      "Epoch: 28/40 Iteration: 6800 | Train loss: 0.00004\n",
      "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00002\n",
      "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00002\n",
      "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00005\n",
      "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00003\n",
      "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00002\n",
      "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00006\n",
      "Epoch: 28/40 Iteration: 6960 | Train loss: 0.00000\n",
      "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00003\n",
      "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00000\n",
      "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00006\n",
      "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00002\n",
      "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00001\n",
      "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00001\n",
      "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00001\n",
      "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00003\n",
      "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00005\n",
      "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00003\n",
      "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00001\n",
      "Epoch: 30/40 Iteration: 7380 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00001\n",
      "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00001\n",
      "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00002\n",
      "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00000\n",
      "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00050\n",
      "Epoch: 30/40 Iteration: 7500 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7520 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00000\n",
      "Epoch: 31/40 Iteration: 7560 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00009\n",
      "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00003\n",
      "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00000\n",
      "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7680 | Train loss: 0.00002\n",
      "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00000\n",
      "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00001\n",
      "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00000\n",
      "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00002\n",
      "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00005\n",
      "Epoch: 32/40 Iteration: 7940 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00001\n",
      "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00006\n",
      "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00007\n",
      "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00007\n",
      "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8080 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8120 | Train loss: 0.00002\n",
      "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00001\n",
      "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00000\n",
      "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00000\n",
      "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00000\n",
      "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00003\n",
      "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00018\n",
      "Epoch: 34/40 Iteration: 8360 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00002\n",
      "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00000\n",
      "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00006\n",
      "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00007\n",
      "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00001\n",
      "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00000\n",
      "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00001\n",
      "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00000\n",
      "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00001\n",
      "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00000\n",
      "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00004\n",
      "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00000\n",
      "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00001\n",
      "Epoch: 37/40 Iteration: 9180 | Train loss: 0.04381\n",
      "Epoch: 37/40 Iteration: 9200 | Train loss: 0.15328\n",
      "Epoch: 37/40 Iteration: 9220 | Train loss: 0.09405\n",
      "Epoch: 37/40 Iteration: 9240 | Train loss: 0.02051\n",
      "Epoch: 38/40 Iteration: 9260 | Train loss: 0.10751\n",
      "Epoch: 38/40 Iteration: 9280 | Train loss: 0.01811\n",
      "Epoch: 38/40 Iteration: 9300 | Train loss: 0.03967\n",
      "Epoch: 38/40 Iteration: 9320 | Train loss: 0.07729\n",
      "Epoch: 38/40 Iteration: 9340 | Train loss: 0.02859\n",
      "Epoch: 38/40 Iteration: 9360 | Train loss: 0.05374\n",
      "Epoch: 38/40 Iteration: 9380 | Train loss: 0.05944\n",
      "Epoch: 38/40 Iteration: 9400 | Train loss: 0.16259\n",
      "Epoch: 38/40 Iteration: 9420 | Train loss: 0.03607\n",
      "Epoch: 38/40 Iteration: 9440 | Train loss: 0.01499\n",
      "Epoch: 38/40 Iteration: 9460 | Train loss: 0.01970\n",
      "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00087\n",
      "Epoch: 38/40 Iteration: 9500 | Train loss: 0.02273\n",
      "Epoch: 39/40 Iteration: 9520 | Train loss: 0.05159\n",
      "Epoch: 39/40 Iteration: 9540 | Train loss: 0.01552\n",
      "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00981\n",
      "Epoch: 39/40 Iteration: 9580 | Train loss: 0.05864\n",
      "Epoch: 39/40 Iteration: 9600 | Train loss: 0.00198\n",
      "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00216\n",
      "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00118\n",
      "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00127\n",
      "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00139\n",
      "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00033\n",
      "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00214\n",
      "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00179\n",
      "Epoch: 40/40 Iteration: 9760 | Train loss: 0.02597\n",
      "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00231\n",
      "Epoch: 40/40 Iteration: 9800 | Train loss: 0.01474\n",
      "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00030\n",
      "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00078\n",
      "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00215\n",
      "Epoch: 40/40 Iteration: 9880 | Train loss: 0.00473\n",
      "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00016\n",
      "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00073\n",
      "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00087\n",
      "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00016\n",
      "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00075\n",
      "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00065\n"
     ]
    }
   ],
   "source": [
    "rnn.train(X_train, y_train, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Test: \n",
    "preds = rnn.predict(X_test)\n",
    "y_true = y_test[:len(preds)]\n",
    "print('Test Acc.: %.3f' % (\n",
    "      np.sum(preds == y_true) / len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get probabilities:\n",
    "proba = rnn.predict(X_test, return_proba=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
